{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Resnet18.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "vIo5bKuJCVHy",
        "colab_type": "code",
        "outputId": "81ce42a2-0f20-4aac-b385-dc3bd02b3035",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# Load the Drive helper and mount\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "c71k1UKdDLkl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch \n",
        "import numpy as np \n",
        "import torchvision.transforms as transforms \n",
        "import torchvision \n",
        "import torch.nn.functional as F\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import torch.nn as nn\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KMtwSImgD-Yu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "with open(\"/content/drive/My Drive/CV/train_labels.csv\",\"r\") as files:\n",
        "  train_labels = [int(i) for i in files.read().split(\",\")]\n",
        "  train_labels = np.array(train_labels)-1\n",
        "with open(\"/content/drive/My Drive/CV/test_labels.csv\",\"r\") as files:\n",
        "  test_labels = [int(i) for i in files.read().split(\",\")]\n",
        "  test_labels = np.array(test_labels)-1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "o3fveNuEqY-Q",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def conv3(in_planes, out_planes, stride=1, groups=1):\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
        "                     padding=1, groups=groups, bias=False)\n",
        "\n",
        "\n",
        "def conv1(in_planes, out_planes, stride=1):\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "isGHIG2ZqYtR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Resunit(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n",
        "                 base_width=64, norm_layer=None):\n",
        "        super(Resunit, self).__init__()\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        if groups != 1 or base_width != 64:\n",
        "            raise ValueError('Resunit only supports groups=1 and base_width=64')\n",
        "        self.conv1 = conv3(inplanes, planes, stride)\n",
        "        self.bn1 = norm_layer(planes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = conv3(planes, planes)\n",
        "        self.bn2 = norm_layer(planes)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "G-cpDHwhqiqG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class ResNet(nn.Module):\n",
        "\n",
        "    def __init__(self, block, layers, num_classes=8, zero_init_residual=False,\n",
        "                 groups=1, width_per_group=64, norm_layer=None):\n",
        "        super(ResNet, self).__init__()\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "\n",
        "        self.inplanes = 64\n",
        "        self.groups = groups\n",
        "        self.base_width = width_per_group\n",
        "        self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=7, stride=2, padding=3,\n",
        "                               bias=False)\n",
        "        self.bn1 = norm_layer(self.inplanes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        self.layer1 = self._make_layer(block, 64, layers[0], norm_layer=norm_layer)\n",
        "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2, norm_layer=norm_layer)\n",
        "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2, norm_layer=norm_layer)\n",
        "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2, norm_layer=norm_layer)\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "        if zero_init_residual:\n",
        "            for m in self.modules():\n",
        "                if isinstance(m, Bottleneck):\n",
        "                    nn.init.constant_(m.bn3.weight, 0)\n",
        "                elif isinstance(m, Resunit):\n",
        "                    nn.init.constant_(m.bn2.weight, 0)\n",
        "\n",
        "    def _make_layer(self, block, planes, blocks, stride=1, norm_layer=None):\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        downsample = None\n",
        "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                conv1(self.inplanes, planes * block.expansion, stride),\n",
        "                norm_layer(planes * block.expansion),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(block(self.inplanes, planes, stride, downsample, self.groups,\n",
        "                            self.base_width, norm_layer))\n",
        "        self.inplanes = planes * block.expansion\n",
        "        for _ in range(1, blocks):\n",
        "            layers.append(block(self.inplanes, planes, groups=self.groups,\n",
        "                                base_width=self.base_width, norm_layer=norm_layer))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "def resnet18(pretrained=False, **kwargs):\n",
        "    model = ResNet(Resunit, [2, 2, 2, 2], **kwargs)\n",
        "    if pretrained:\n",
        "        model.load_state_dict(model_zoo.load_url(model_urls['resnet18']))\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_HhE_TKyU5Jm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Imagedata(torch.utils.data.Dataset):\n",
        "  def __init__(self,path,labels,transform):\n",
        "    self.path=path\n",
        "    self.labels=labels\n",
        "    self.tr=transform\n",
        "  def __getitem__(self,index):\n",
        "    images = Image.open(self.path+str(index+1)+\".jpg\")\n",
        "    tr_images = self.tr(images)\n",
        "    y = self.labels[index]\n",
        "    p = {\"img\":tr_images,\"label\":y}\n",
        "    return p\n",
        "  def __len__(self):\n",
        "    return self.labels.shape[0]\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "a1gk4rPfEJ1f",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def test():\n",
        "        #To convert data from PIL to tensor\n",
        "    transform = transforms.Compose(\n",
        "        [transforms.Resize((224,224)),transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",
        "        )\n",
        "\n",
        "    train = Imagedata(\"/content/drive/My Drive/CV/train/\",train_labels,transform = transform)\n",
        "    trainset = torch.utils.data.DataLoader(train,batch_size=64,shuffle=True,num_workers=2)\n",
        "    \n",
        "    test = Imagedata(\"/content/drive/My Drive/CV/test/\",test_labels,transform = transform)\n",
        "    testset = torch.utils.data.DataLoader(test,batch_size=64,shuffle=True,num_workers=2)\n",
        "    #Load train and test set:\n",
        "#     train = torchvision.datasets.CIFAR10(root='./data',train=True,download=True,transform=transform)\n",
        "#     trainset = torch.utils.data.DataLoader(train,batch_size=128,shuffle=True)\n",
        "\n",
        "#     test = torchvision.datasets.CIFAR10(root='./data',train=False,download=True,transform=transform)\n",
        "#     testset = torch.utils.data.DataLoader(test,batch_size=128,shuffle=False)\n",
        "    \n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(device)\n",
        "\n",
        "    net = resnet18()\n",
        "    net.to(device)\n",
        "    costFunc = torch.nn.CrossEntropyLoss()\n",
        "    optimizer =  torch.optim.Adam(net.parameters(),lr=0.002,weight_decay=1e-3)\n",
        "\n",
        "    for epoch in range(20):\n",
        "        closs = 0\n",
        "        total=0\n",
        "        correctHits=0\n",
        "        for i in trainset:\n",
        "            data = i[\"img\"].to(device)\n",
        "            output = i[\"label\"].to(device)\n",
        "            prediction = net(data)\n",
        "            loss = costFunc(prediction,output)\n",
        "            closs = loss.item()\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            _,prediction = torch.max(prediction.data,1) \n",
        "            total += output.size(0)\n",
        "            correctHits += (prediction==output).sum().item()\n",
        "        print(\"Training  :- for Epoch\",epoch+1,\"Loss is:\",loss.item())\n",
        "    print('Training Accuracy = '+str((correctHits/total)*100))\n",
        "    correctHits=0\n",
        "    total=0\n",
        "    for i in testset:\n",
        "        data = i[\"img\"].to(device)\n",
        "        output = i[\"label\"].to(device)\n",
        "        prediction = net(data)\n",
        "        _,prediction = torch.max(prediction.data,1) \n",
        "        total += output.size(0)\n",
        "        correctHits += (prediction==output).sum().item()\n",
        "    print('Test Accuracy = '+str((correctHits/total)*100))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "n9-ByKsJEgFW",
        "colab_type": "code",
        "outputId": "8f0757e2-155c-4956-8fe0-ba6e739efb6e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 403
        }
      },
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    test()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda:0\n",
            "Training  :- for Epoch 1 Loss is: 1.288830280303955\n",
            "Training  :- for Epoch 2 Loss is: 1.0134836435317993\n",
            "Training  :- for Epoch 3 Loss is: 0.8520622253417969\n",
            "Training  :- for Epoch 4 Loss is: 0.7764910459518433\n",
            "Training  :- for Epoch 5 Loss is: 0.7935084104537964\n",
            "Training  :- for Epoch 6 Loss is: 0.8605446815490723\n",
            "Training  :- for Epoch 7 Loss is: 0.923723578453064\n",
            "Training  :- for Epoch 8 Loss is: 0.575831413269043\n",
            "Training  :- for Epoch 9 Loss is: 0.5524840950965881\n",
            "Training  :- for Epoch 10 Loss is: 0.3909381926059723\n",
            "Training  :- for Epoch 11 Loss is: 0.8550170660018921\n",
            "Training  :- for Epoch 12 Loss is: 0.8101996183395386\n",
            "Training  :- for Epoch 13 Loss is: 1.0629962682724\n",
            "Training  :- for Epoch 14 Loss is: 0.3212195038795471\n",
            "Training  :- for Epoch 15 Loss is: 0.3913416266441345\n",
            "Training  :- for Epoch 16 Loss is: 0.49861130118370056\n",
            "Training  :- for Epoch 17 Loss is: 0.3664094805717468\n",
            "Training  :- for Epoch 18 Loss is: 0.4410684406757355\n",
            "Training  :- for Epoch 19 Loss is: 0.1922570914030075\n",
            "Training  :- for Epoch 20 Loss is: 0.493949830532074\n",
            "Training Accuracy = 85.59322033898306\n",
            "Test Accuracy = 82.125\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "nRdn7ncBbel3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}